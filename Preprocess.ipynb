{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview\n",
    "# Step 0: Combine train set and comptition test set\n",
    "# Step 1: Drop unimportant data\n",
    "# Step 2: Normalization\n",
    "# Step 3: One-hot encode the categorical features\n",
    "# Step 4: Separate train set and comptition test set\n",
    "#         Save and drop feature: building id\n",
    "# Step 5: Split the data into train, test data\n",
    "# Step 6: Copy and oversample train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train_values.csv')\n",
    "label = pd.read_csv('data/train_labels.csv')\n",
    "test_values = pd.read_csv('data/test_values.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Combine train set and comptition test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = data.append(test_values).set_index(keys = 'building_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Drop nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Purpose for dropping:\n",
    "geo_level_2_id : Reduce computational cost\n",
    "\n",
    "geo_level_3_id : Reduce computational cost\n",
    "\n",
    "has_superstructure_adobe_mud : Unimportant feature \n",
    "\n",
    "has_superstructure_mud_mortar_brick : Unimportant feature \n",
    "\n",
    "has_superstructure_stone_flag: Unimportant feature\n",
    "\n",
    "has_superstructure_timber : Unimportant feature\n",
    "\n",
    "has_superstructure_bamboo : Unimportant feature\n",
    "\n",
    "has_superstructure_other : Unimportant feature\n",
    "\n",
    "has_secondary_use : Information of this feature is already represented by other has_secondary_use_xxx features\n",
    "\n",
    "land_surface_condition : Unimportant feature\n",
    "\n",
    "position : Unimportant feature\n",
    "\n",
    "count_floors_pre_eq: Highly correlated with height_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['geo_level_2_id', 'geo_level_3_id', 'has_superstructure_adobe_mud',\\\n",
    "           'has_superstructure_mud_mortar_brick', 'has_superstructure_stone_flag', 'has_superstructure_timber',\\\n",
    "           'has_superstructure_bamboo','has_superstructure_other',\\\n",
    "           'has_secondary_use', 'land_surface_condition', 'position','count_floors_pre_eq']\n",
    "\n",
    "to_enc = ['geo_level_1_id', 'foundation_type', 'roof_type', 'ground_floor_type', 'other_floor_type',\\\n",
    "          'plan_configuration', 'legal_ownership_status']\n",
    "\n",
    "num_col = ['age', 'area_percentage', 'height_percentage', 'count_families']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.drop(columns=to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Min-max scale both datasets (Normalization)\n",
    "\n",
    "#### Purpose of Min-max scale: Reduce weight of numeric features(features with high magnitudes) in model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-max scale the training data_keepsets\n",
    "#Min-max scale test_values\n",
    "full_data[num_col] = full_data[num_col].astype('float')\n",
    "scaler = MinMaxScaler()\n",
    "full_data[num_col] = scaler.fit_transform(full_data[num_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: One hot encode the categorical features and preserve building id from test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: One hot encode the categorical features\n",
    "full_data = pd.get_dummies(full_data, prefix=to_enc, columns=to_enc, dtype='bool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Separate train set and comptition test set. Save and remove feature: building id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_building_id = data['building_id']\n",
    "test_building_id = test_values['building_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = full_data.loc[train_building_id]\n",
    "test_values = full_data.loc[test_building_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(drop=True, inplace=True)\n",
    "test_values.reset_index(drop=True, inplace=True)\n",
    "\n",
    "label = label['damage_grade']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Split train dataset into train, test\n",
    "#### The test set is used to check how well the learning model generalises to data it has not seen before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose your test size=0.2\n",
    "#Split both oversampled and non-oversampled data\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(data, label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Copy and oversample train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=0)\n",
    "train_x_over, train_y_over = ros.fit_resample(train_x, train_y)\n",
    "test_x_over = test_x.copy()\n",
    "test_y_over = test_y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
